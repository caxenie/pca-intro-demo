{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Principal Component Analysis (PCA) – Grundlagen und Anwendungen\n",
    "\n",
    "![](repository-address.png)\n",
    "\n",
    "Codebase: https://github.com/caxenie/pca-intro-demo\n",
    "\n",
    "Basiert auf: https://mml-book.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "jupyter nbconvert PCAvsDVAEde.ipynb --to slides --post serve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "![](setup.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](setup-sensors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](setup-sensors-data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](setup-sensors-data-kinematic-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grundlegende Mathematik der PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lineare Transformation\n",
    "\n",
    "Es sei $\\{x_i\\}^N_{i=1}$ eine Menge von $N$ Beobachtungs-Vektoren der Dimension $n$ mit $n\\leq N$.\n",
    "\n",
    "Eine __lineare Transformation__ eines __endlich-dimensionalen__ Vektors kann als __Matrix Multiplikation__ ausgedrückt werden: \n",
    "\n",
    "$$ \\begin{align} y_i = W x_i \\end{align} $$  \n",
    "  \n",
    "mit $x_i \\in R^{n}, y_i \\in R^{m}$ und $W \\in R^{nxm}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Lineare Transformation\n",
    "\n",
    "* Das $j-te$ Element in $y_i$ ist das __Innere Produkt__ von $x_i$ und der $j-ten$ Spalte der Matrix $W$, welche wir durch $w_j$ bezeichen. Es sei $X \\in R^{nxN}$ die Matrix, welche wir durch horizontale Aneinanderreihung der Vektoren $\\{x_i\\}^N_{i=1}$ erhalten, \n",
    "\n",
    "$$ X = \\begin{bmatrix} | ... | \\\\ x_1 ... x_N \\\\ | ... | \\end{bmatrix} $$\n",
    "\n",
    "* Aus der __linearen Transformation__ folgt:\n",
    "\n",
    "$$ Y = W^TX,  Y_0 = W^TX_0, $$\n",
    "\n",
    "wobei $X_0$ die __Matrix der zentrierten Elemente__ (d.h. wir subtrahieren den Mittelwert von jeder Beobachtung) bezeichnet, und __Kovarianzmatrix__ $X_0X_0^T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionsreduzierung, Komprimierung\n",
    "\n",
    "PCA wird zur __Dimensions-Reduktion__ verwendet, da sie durch die durch eine lineare Transformation die __Anzahl der Variablen reduziert__. \n",
    "\n",
    "Da nur die ersten __$m$ Hauptkomponenten erhalten__ werden, __verliert__ PCA __information__ (d.h. __verlustreiche Komprimierung__). \n",
    "\n",
    "Der __Verlust__ ( _Summe des quadratischen Rekonstruktions-Fehlers_ ) wird jedoch durch die __Maximierung der Komponenten-Varianzen minimiert__\n",
    "\n",
    "$$ \\min_{W \\in R^{nxm}} \\| X_0 - WW^TX_0 \\|_F^2, W^TW = I_{mxm}$$\n",
    "\n",
    "wobei $F$ die Frobenius-Norm bezeichnet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](setup-sensors-data-kinematic-chain.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundlegende PCA\n",
    "\n",
    "Jetzt werden wir PCA implementieren. Bevor wir das tun, sollten wir einen Moment innehalten und\n",
    "über die Schritte zur Durchführung der PCA nachdenken. Angenommen, wir führen eine PCA für\n",
    "einem Datensatz $\\boldsymbol X$ für $M$ Hauptkomponenten durchführen.\n",
    "Dann müssen wir die folgenden Schritte durchführen, die wir in Teile unterteilen:\n",
    "\n",
    "\n",
    "1. die Normalisierung und Standardisierung der Daten(` normalisieren`)\n",
    "2. die Eigenwerte und die entsprechenden Eigenvektoren für die Kovarianzmatrix $S$ finden.\n",
    "Sortieren Sie nach den größten Eigenwerten und den zugehörigen Eigenvektoren(`eig`)\n",
    "3. Anschließend können wir die Projektion und Rekonstruktion der Daten auf den von den obersten $n$-Eigenvektoren aufgespannten Raum berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \"\"\"Normalisierung des gegebenen Datensatzes X\n",
    "        \n",
    "    Args:\n",
    "        X: ndarray, Datensatz\n",
    "    \n",
    "    Returns:\n",
    "        (Xbar, mean, std): Tupel von ndarray, Xbar ist der normalisierte Datensatz\n",
    "        mit Mittelwert 0 und Standardabweichung 1; mean und std sind der\n",
    "        Mittelwert bzw. die Standardabweichung.\n",
    "    \n",
    "    Note:\n",
    "        Sie werden auf Dimensionen stoßen, bei denen die Standardabweichung\n",
    "        Null ist, für die bei einer Normalisierung die normalisierten Daten\n",
    "        NaN sein. Behandeln Sie dies, indem Sie `std = 1` für diese\n",
    "        Dimensionen bei der Normalisierung.\n",
    "    \"\"\"\n",
    "    mu = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    std_filled = std.copy()\n",
    "    std_filled[std==0] = 1.\n",
    "    Xbar = ((X-mu)/std_filled)\n",
    "    return Xbar, mu, std\n",
    "\n",
    "def eig(S):\n",
    "    \"\"\"Berechnen Sie die Eigenwerte und die entsprechenden Eigenvektoren für \n",
    "        die Kovarianzmatrix S.\n",
    "        \n",
    "    Args:\n",
    "        S: nDarray, Kovarianzmatrix\n",
    "    \n",
    "    Returns:\n",
    "        (eigvals, eigvecs): ndarray, die Eigenwerte und Eigenvektoren\n",
    "\n",
    "    Note:\n",
    "        Die Eigenwerte und Eigenvektoren sollten in absteigender\n",
    "        Reihenfolge der Eigenwerte\n",
    "    \"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eig(S)\n",
    "    k = np.argsort(eigvals)[::-1]\n",
    "    return eigvals[k], eigvecs[:,k]\n",
    "\n",
    "def projection_matrix(B):\n",
    "    \"\"\"Berechnen Sie die Projektionsmatrix auf den von \"B\" aufgespannten Raum\n",
    "    \n",
    "    Args:\n",
    "        B: nDarray der Dimension (D, M), die Basis für den Unterraum\n",
    "    \n",
    "    Returns:\n",
    "        P: die Projektionsmatrix\n",
    "    \"\"\"\n",
    "    return (B @ np.linalg.inv(B.T @ B) @ B.T)\n",
    "\n",
    "def PCA(X, num_components):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: nDarray der Größe (N, D), wobei D die Dimension der Daten ist,\n",
    "            und N die Anzahl der Datenpunkte ist\n",
    "       num_components: die Anzahl der zu verwendenden Hauptkomponenten.\n",
    "    Returns:\n",
    "        X_reconstruct: ndarray der Rekonstruktion\n",
    "        von X aus den ersten `num_components` Hauptkomponenten.\n",
    "    \"\"\"\n",
    "    # zunächst werden die Ziffern normalisiert, so dass sie einen Mittelwert \n",
    "    # von Null und eine Einheitsvarianz haben.\n",
    "    \n",
    "    # berechnen Sie dann die Kovarianzmatrix S der Daten\n",
    "    S = 1.0/len(X) * np.dot(X.T, X)\n",
    "\n",
    "    # als nächstes werden die Eigenwerte und die entsprechenden Eigenvektoren für S\n",
    "    eig_vals, eig_vecs = eig(S)\n",
    "\n",
    "    # Indizes für die größten Eigenwerte zu finden, sie zum Sortieren der Eigenwerte \n",
    "    # zu verwenden und entsprechende Eigenvektoren.\n",
    "    eig_vals, eig_vecs = eig_vals[:num_components], eig_vecs[:, :num_components]\n",
    "\n",
    "    # Dimensionalitätsreduktion der Originaldaten\n",
    "    B = np.real(eig_vecs)\n",
    "    \n",
    "    # Z = X.T.dot(W)\n",
    "    # die Bilder aus der niederdimensionalen Darstellung rekonstruieren\n",
    "    reconst = (projection_matrix(B) @ X.T)\n",
    "    return reconst.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Skalierung\n",
    "\n",
    "Zur Berechung der PCA können viele verschiedene __iterative Algorithmen__ eingesetzt werden \n",
    "* QR Algorithmen\n",
    "* Jacobi Algorithmus\n",
    "* Power methode\n",
    "* Singulärwert-Zerlegung (Singular Value Decomposition, SVD)\n",
    "\n",
    "Für __sehr große Datenmengen__ eignen sich diese Algorithmen __nicht__!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA \n",
    "\n",
    "![](svd-graphic-simple.png)\n",
    "* Wird zur __Dimensions-Reduzierung__ genutzt (Komprimierung)\n",
    "* Die __Rekonstruktion der Beobachtungen__(\"decoding\") aus den führenden __Hauptkomponenten__ hat den __niedrigsten quadratischen Fehler__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Eine intuitive Perspektive ... für hochdimensionale Daten\n",
    "\n",
    "#### \"... realistische, hochdimensionale Daten konzentrieren sich in der Nähe einer nichtlinearen, niedrigdimensionalen Mannigfaltigkeit ...\" [Lei et al., 2018]\n",
    "\n",
    "![](manifold.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA an Hand von Beispielen\n",
    "\n",
    "Der __MNIST (Modified National Institute of Standards and Technology) Datensatz__ von handgeschriebenen Zahlen besteht aus __60,000 Trainings- und 10,000 Test-Beispielen__. Die Zahlen wurden hinsichtlich Ihrer Größe __normalisiert und in einem Bild fester Größe zentriert__.\n",
    "\n",
    "![](mnist.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML \n",
    "<style>\n",
    "td {\n",
    "  font-size: 15px\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Singulärwert-Zerlegung \n",
    "### (Singular Value Decomposition, SVD)\n",
    "\n",
    "Ein Vektor $v$ der Dimension $N$ ist ein __Eigenvektor__ einer quadratischen N × N Matrix $A$, wenn diese die folgende __lineare Gleichung__ erfüllt\n",
    "\n",
    "$$Av =\\lambda v$$\n",
    "\n",
    "wobei $λ$ ein skalarer Wert ist, welcher als der __zum Eigenvektor v gehörende Eigenwert__ bezeichnet wird."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Singulärwert-Zerlegung \n",
    "### (Singular Value Decomposition, SVD)\n",
    "\n",
    "Die Matrix $Y_0 \\in R^{nxN}$ kann __faktorisert__ werden als $Y_0 = U \\Sigma V^T$, wobei $U \\in R^{nxn}$ und $V \\in R^{NxN}$ __orthogonale Matrizen__ sind und $\\Sigma \\in R^{nxN}$ abgesehen von der Diagonalwerten (den sogenannten __Singulär-Werten__) nur aus Nullen besteht.\n",
    "\n",
    "Die Singulärwertzerlegung von $Y_0$ ist äquivalent zur __Eigenwertzerlegung__ von $Y_0T_0^T$. \n",
    "\n",
    "![](svd-graphic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live Coding Demo\n",
    "\n",
    "\n",
    "basierend auf dem MML-Buch von Marc Deisenroth und Yicheng Luo\n",
    "https://mml-book.com\n",
    "\n",
    "\n",
    "1. Code schreiben, der PCA implementiert.\n",
    "2. Code schreiben, der die PCA für hochdimensionale Datensätze implementiert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from ipywidgets import interact\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits(n_class=10)\n",
    "X, y = digits.data, digits.target\n",
    "images, labels = digits.data, digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "fig = plt.figure(figsize=(6, 6))  # figure size in inches\n",
    "fig.subplots_adjust(left=0, right=1, bottom=0, top=1, hspace=0.05, wspace=0.05)\n",
    "\n",
    "for i in range(64):\n",
    "    ax = fig.add_subplot(8, 8, i + 1, xticks=[], yticks=[])\n",
    "    ax.imshow(digits.images[i], cmap=plt.cm.binary, interpolation='nearest')\n",
    "    # label the image with the target value\n",
    "    ax.text(0, 7, str(digits.target[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "proj = pca.fit_transform(digits.data)\n",
    "plt.scatter(proj[:, 0], proj[:, 1], c=digits.target, cmap=\"Paired\")\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot a digit from the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(images[0].reshape(8,8), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grundlegende PCA\n",
    "\n",
    "Jetzt werden wir PCA implementieren. Bevor wir das tun, sollten wir einen Moment innehalten und\n",
    "über die Schritte zur Durchführung der PCA nachdenken. Angenommen, wir führen eine PCA für\n",
    "einem Datensatz $\\boldsymbol X$ für $M$ Hauptkomponenten durchführen.\n",
    "Dann müssen wir die folgenden Schritte durchführen, die wir in Teile unterteilen:\n",
    "\n",
    "\n",
    "1. die Normalisierung und Standardisierung der Daten(` normalisieren`)\n",
    "2. die Eigenwerte und die entsprechenden Eigenvektoren für die Kovarianzmatrix $S$ finden.\n",
    "Sortieren Sie nach den größten Eigenwerten und den zugehörigen Eigenvektoren(`eig`)\n",
    "3. Anschließend können wir die Projektion und Rekonstruktion der Daten auf den von den obersten $n$-Eigenvektoren aufgespannten Raum berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def normalize(X):\n",
    "    \"\"\"Normalisierung des gegebenen Datensatzes X\n",
    "        \n",
    "    Args:\n",
    "        X: ndarray, Datensatz\n",
    "    \n",
    "    Returns:\n",
    "        (Xbar, mean, std): Tupel von ndarray, Xbar ist der normalisierte Datensatz\n",
    "        mit Mittelwert 0 und Standardabweichung 1; mean und std sind der\n",
    "        Mittelwert bzw. die Standardabweichung.\n",
    "    \n",
    "    Note:\n",
    "        Sie werden auf Dimensionen stoßen, bei denen die Standardabweichung\n",
    "        Null ist, für die bei einer Normalisierung die normalisierten Daten\n",
    "        NaN sein. Behandeln Sie dies, indem Sie `std = 1` für diese\n",
    "        Dimensionen bei der Normalisierung.\n",
    "    \"\"\"\n",
    "    mu = np.mean(X, axis=0)\n",
    "    std = np.std(X, axis=0)\n",
    "    std_filled = std.copy()\n",
    "    std_filled[std==0] = 1.\n",
    "    Xbar = ((X-mu)/std_filled)\n",
    "    return Xbar, mu, std\n",
    "\n",
    "def eig(S):\n",
    "    \"\"\"Berechnen Sie die Eigenwerte und die entsprechenden Eigenvektoren für \n",
    "        die Kovarianzmatrix S.\n",
    "        \n",
    "    Args:\n",
    "        S: nDarray, Kovarianzmatrix\n",
    "    \n",
    "    Returns:\n",
    "        (eigvals, eigvecs): ndarray, die Eigenwerte und Eigenvektoren\n",
    "\n",
    "    Note:\n",
    "        Die Eigenwerte und Eigenvektoren sollten in absteigender\n",
    "        Reihenfolge der Eigenwerte\n",
    "    \"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eig(S)\n",
    "    k = np.argsort(eigvals)[::-1]\n",
    "    return eigvals[k], eigvecs[:,k]\n",
    "\n",
    "def projection_matrix(B):\n",
    "    \"\"\"Berechnen Sie die Projektionsmatrix auf den von \"B\" aufgespannten Raum\n",
    "    \n",
    "    Args:\n",
    "        B: nDarray der Dimension (D, M), die Basis für den Unterraum\n",
    "    \n",
    "    Returns:\n",
    "        P: die Projektionsmatrix\n",
    "    \"\"\"\n",
    "    return (B @ np.linalg.inv(B.T @ B) @ B.T)\n",
    "\n",
    "def PCA(X, num_components):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X: nDarray der Größe (N, D), wobei D die Dimension der Daten ist,\n",
    "            und N die Anzahl der Datenpunkte ist\n",
    "       num_components: die Anzahl der zu verwendenden Hauptkomponenten.\n",
    "    Returns:\n",
    "        X_reconstruct: ndarray der Rekonstruktion\n",
    "        von X aus den ersten `num_components` Hauptkomponenten.\n",
    "    \"\"\"\n",
    "    # zunächst werden die Ziffern normalisiert, so dass sie einen Mittelwert \n",
    "    # von Null und eine Einheitsvarianz haben.\n",
    "    \n",
    "    # berechnen Sie dann die Kovarianzmatrix S der Daten\n",
    "    S = 1.0/len(X) * np.dot(X.T, X)\n",
    "\n",
    "    # als nächstes werden die Eigenwerte und die entsprechenden Eigenvektoren für S\n",
    "    eig_vals, eig_vecs = eig(S)\n",
    "\n",
    "    # Indizes für die größten Eigenwerte zu finden, sie zum Sortieren der Eigenwerte \n",
    "    # zu verwenden und entsprechende Eigenvektoren.\n",
    "    eig_vals, eig_vecs = eig_vals[:num_components], eig_vecs[:, :num_components]\n",
    "\n",
    "    # Dimensionalitätsreduktion der Originaldaten\n",
    "    B = np.real(eig_vecs)\n",
    "    \n",
    "    # Z = X.T.dot(W)\n",
    "    # die Bilder aus der niederdimensionalen Darstellung rekonstruieren\n",
    "    reconst = (projection_matrix(B) @ X.T)\n",
    "    return reconst.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Einige Vorverarbeitungen der Daten\n",
    "NUM_DATAPOINTS = 1000\n",
    "X = (images.reshape(-1, 8 * 8)[:NUM_DATAPOINTS]) / 255.\n",
    "Xbar, mu, std = normalize(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_component in range(1, 20):\n",
    "    from sklearn.decomposition import PCA as SKPCA\n",
    "    # Wir können eine Standardlösung berechnen, die von der \n",
    "    # PCA-Implementierung von scikit-learn vorgegeben wird\n",
    "    pca = SKPCA(n_components=num_component, svd_solver='full')\n",
    "    sklearn_reconst = pca.inverse_transform(pca.fit_transform(Xbar))\n",
    "    reconst = PCA(Xbar, num_component)\n",
    "    np.testing.assert_almost_equal(reconst, sklearn_reconst)\n",
    "    print(np.square(reconst - sklearn_reconst).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predict, actual):\n",
    "    \"\"\"Hilfsfunktion zur Berechnung des mittleren quadratischen Fehlers (MSE)\"\"\"\n",
    "    return np.square(predict - actual).sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = []\n",
    "reconstructions = []\n",
    "\n",
    "# Iteration über verschiedene Anzahlen von Hauptkomponenten und Berechnung des MSE\n",
    "for num_component in range(1, 100):\n",
    "    reconst = PCA(Xbar, num_component)\n",
    "    error = mse(reconst, Xbar)\n",
    "    reconstructions.append(reconst)\n",
    "    # print('n = {:d}, reconstruction_error = {:f}'.format(num_component, error))\n",
    "    loss.append((num_component, error))\n",
    "\n",
    "reconstructions = np.asarray(reconstructions)\n",
    "reconstructions = reconstructions * std + mu # \"unnormalize\" the reconstructed image\n",
    "loss = np.asarray(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Erstellen einer Tabelle mit der Anzahl der Hauptkomponenten und dem MSE\n",
    "pd.DataFrame(loss).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative Bewertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss[:,0], loss[:,1]);\n",
    "ax.axhline(100, linestyle='--', color='r', linewidth=2)\n",
    "ax.xaxis.set_ticks(np.arange(1, 100, 5));\n",
    "ax.set(xlabel='num_components', ylabel='MSE', title='MSE vs. Anzahl der Hauptkomponenten');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qualitative Bewertung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(digits.images[5], [8, 8]), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(sklearn_reconst[5], [8, 8]), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(reconst[5], [8, 8]), cmap='gray');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example applications"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Biomedizin\n",
    "\n",
    "![](application1.jpg)\n",
    "\n",
    "Quelle: https://www.nature.com/articles/ncomms12575"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stromversorgungssysteme\n",
    "\n",
    "![](anwendung2.png)\n",
    "\n",
    "Quelle: https://www.mdpi.com/1996-1073/13/22/5896"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Massenspektrometrie in der Chemie\n",
    "\n",
    "![](application3.png)\n",
    "\n",
    "Quelle: https://pubs.acs.org/doi/10.1021/acs.macromol.2c02383"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neurowissenschaft\n",
    "\n",
    "![](application4.jpg)\n",
    "\n",
    "Quelle: https://www.sciencedirect.com/science/article/pii/S0165027022002941"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entwicklung von Arzneimitteln (Drug discovery)\n",
    "\n",
    "![](application5.jpg)\n",
    "\n",
    "Quelle: https://www.sciencedirect.com/science/article/abs/pii/S1359644617300181"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bioinformatik\n",
    "\n",
    "![](application6.jpg)\n",
    "\n",
    "Quelle: https://www.frontiersin.org/journals/bioinformatics/articles/10.3389/fbinf.2022.821861/full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Neuronale Alternativen zur PCA: der Autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Für jeden Eingangsvektor $x$ der Dimension $d$ des kompletten Datensaztes der Länge $n$ generiert das neuronale Netz eine Rekonstruktion $x'$ durch:\n",
    "\n",
    "* __Kodierung der Eingangsdaten__ (d.h. verwende die lineare / nicht-lineare Transformation $g_\\phi(.)$)\n",
    "* dies liefert eine __komprimierte Kodierung__ in der dünnsten Netzwerk-Ebene, $z$\n",
    "* __Dekodierung der komprimierten Eingangsdaten__ durch Anwendung der linearen / nicht-linearen Transformation $f_\\theta(.)$\n",
    "\n",
    "![](autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Die __Parameter $(\\theta, \\phi)$ werden im Verlauf des Training derart optimiert__, dass ein den Eingangsdaten möglichst ähnliches Ergebnis , $x \\approx f_\\theta(g_\\phi(x))$, produziert wird. In anderen Worten: __die Indentitäts-Funktion wird erlernt__ mit __Cross-Entropy (bei sigmoid Aktivierungsfuntionen)__, __mittlere quadratische Fehler (MSE)__ etc.:\n",
    "\n",
    "![](autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Denoising Variational Autoencoders (DVAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Das Funktionsprinzip __unterscheidet sich__ vom grundlegenden Autoencoder dahingehend, dass ein gewisses Maß an __Störrauschen__  (einer __gewissen Wahrscheinlichkeitsverteilung__ folgend) den __Eingangsdaten hinzugefügt wird__ und dass die __verborgenen Ebenen__ dieses Rauschen __ausgleichen muss__ um die Eingangsdaten zu __rekonstruieren__ [Im, Bengio et al., 2017, Kingma et al., 2017].\n",
    "![](denoising-variational-autoencoder.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Für jeden gestörten Eingangsvektor $\\tilde x$ eines originalen Vektors $x$ der Dimension $d$, generiert das neuronale Netz eine Rekonstruktion $x'$ durch:\n",
    "* __Kodierung der Eingangsdaten__, welche die Abbildung als Wahrscheinlichkeit der Schätzung von $z$ unter Verwendung der Eingangsdaten darstellt\n",
    "* dies liefert eine __komprimierte Kodierung in der dünnsten Netzwerk-Ebene__ $z$, welche der Verteilung $q_\\phi(z|x)$ folgt\n",
    "* __Dekodierung der komprimierten Eingangsdaten__ an der Ausgangsebene unter Einhaltung des __Beobachtungs-Modells__ $p_\\theta(x|z)$\n",
    "\n",
    "![](denoising-variational-autoencoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the DVAE\n",
    "# encoder part\n",
    "x_noise = Input(shape=(28,28,1))\n",
    "conv_1 = Conv2D(64,(3, 3), padding='valid',activation='relu')(x_noise)\n",
    "conv_2 = Conv2D(64,(3, 3), padding='valid',activation='relu')(conv_1)\n",
    "pool_1 = MaxPooling2D((2, 2))(conv_2)\n",
    "conv_3 = Conv2D(32,(3, 3), padding='valid',activation='relu')(pool_1)\n",
    "pool_2 = MaxPooling2D((2, 2))(conv_3)\n",
    "h=Flatten()(pool_2)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the DVAE\n",
    "# reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the DVAE\n",
    "# decoder part\n",
    "# we instantiate these layers separately so as to reuse them later\n",
    "z=Reshape([1,1,latent_dim])(z)\n",
    "conv_0T = Conv2DTranspose(128,(1, 1), padding='valid',activation='relu')(z)#1*1\n",
    "conv_1T = Conv2DTranspose(64,(3, 3), padding='valid',activation='relu')(conv_0T)#3*3\n",
    "conv_2T = Conv2DTranspose(64,(3, 3), padding='valid',activation='relu')(conv_1T)#5*5\n",
    "conv_3T = Conv2DTranspose(48,(3, 3), strides=(2, 2),padding='same',activation='relu')(conv_2T)#10*10\n",
    "conv_4T = Conv2DTranspose(48,(3, 3), padding='valid',activation='relu')(conv_3T)#12*12\n",
    "conv_5T = Conv2DTranspose(32,(3, 3), strides=(2, 2),padding='same',activation='relu')(conv_4T)#24*24\n",
    "conv_6T = Conv2DTranspose(16,(3, 3), padding='valid',activation='relu')(conv_5T)#26*26\n",
    "x_out = Conv2DTranspose(1,(3, 3), padding='valid',activation='sigmoid')(conv_6T)#28*28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DVAE\n",
    "\n",
    "* DVAE __Verlustfunktion__ beinhaltet die Erstellung von Beispielen aus $z \\backsim q_\\phi(z|x)$. Dies ist ein __stochastischer Prozess__ und eignet sich daher __nicht zur Fehlerrückführung__.\n",
    "\n",
    "\n",
    "* Die __geschätzte Posteriori-Verteilung $q_\\phi(z|x)$__ approximiert die tatsächliche Verteilung $p_\\theta(z|x)$. \n",
    "\n",
    "\n",
    "* Wir können die __Kullback-Leibler Abweichung__, $D_{KL}$  benutzen um die __Differenz der beiden Verteilungen__ zu quantifizieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the DVAE\n",
    "# reparameterization trick\n",
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## DVAE\n",
    "\n",
    "Durch __Minimierung des Verlusts__, __maximieren__ wir daher die __untere Schranke der Wahrscheinlichkeit__ (__evidence lower bound (ELBO)__) zur Generierung echter Daten-Beispiele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Implement the DVAE\n",
    "# instantiate model\n",
    "dvae = Model(x_noise, x_out)\n",
    "dvae.summary()\n",
    "\n",
    "# Compute loss\n",
    "def DVAE_loss(x_origin,x_out):\n",
    "    x_origin=K.flatten(x_origin)\n",
    "    x_out=K.flatten(x_out)\n",
    "    xent_loss = original_dim * metrics.binary_crossentropy(x_origin, x_out)\n",
    "    kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "    dvae_loss = K.mean(xent_loss + kl_loss)\n",
    "    return dvae_loss\n",
    "\n",
    "# compile the model\n",
    "dvae.compile(optimizer='adam', loss=DVAE_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](network-layout.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Train the DVAE\n",
    "dvae.fit(noise_train,x_train,  shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=(noise_test, x_test))\n",
    "\n",
    "# Comparison PCA vs. DVAE\n",
    "# testing the DVAE\n",
    "num_test=10000\n",
    "showidx=np.random.randint(0,num_test,n_images)\n",
    "x_out=dvae.predict(x_test[showidx])\n",
    "\n",
    "# prepare data for testing PCA\n",
    "pcaInputTest = np.reshape(x_test,[shape_x_test[0],shape_x_test[1]*shape_x_test[2]]).astype('float32')/255\n",
    "pcaOutput = analytical_pca(pcaInputTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "![](training-progress.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "digit_size = 28\n",
    "figure = np.zeros((digit_size * 4, digit_size * n_images))\n",
    "for i,idx in enumerate (showidx):\n",
    "    figure[0: 28,i *28: (i + 1) * 28] = np.reshape(x_test[idx], [28, 28]) # input data\n",
    "    figure[28: 28 * 2,i *28: (i + 1) * 28] = np.reshape(noise_test[idx], [28, 28]) # noisy input data \n",
    "    figure[28 * 2: 28 * 3,i *28: (i + 1) * 28] = np.reshape(x_out[i], [28, 28]) # DVAE output\n",
    "    figure[28 * 3: 28 * 4,i *28: (i + 1) * 28] = np.reshape(pcaOutput[idx], [28, 28]) # PCA output\n",
    "plt.figure(figsize=(28 * 4, 28*n_images))\n",
    "plt.imshow(figure, cmap='Greys_r')\n",
    "# plt.savefig('inference_output.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%HTML \n",
    "<style>\n",
    "td {\n",
    "  font-size: 15px\n",
    "}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vergleich von PCA und DVAE\n",
    "\n",
    "### Inferenz\n",
    "\n",
    "![](inference_output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA vs. Autoencoders\n",
    "## \"Zwei identische Fremde\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "jp-MarkdownHeadingCollapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# PCA vs. Autoencoders\n",
    "\n",
    "*  Ein __Autoencoder__ mit einer einzelnen __voll verbundenen (fully-connected) versteckten Ebene__, einer __linearen Aktivierungsfunktion__ und dem __quadratischen Fehler als Kostenfunktion__ ist __eng mit der PCA verwandt__ - seine __Gewichten__ spannen den __Untervektorraum der Hauptkomponenten__ auf [Plaut, 2018]\n",
    "\n",
    "\n",
    "* Bei __Autoencodern__ sorgt die __diagonale Approximation beim Kodiervorgang__ zusammen mit der __inhärenten Stochastizität__ für lokale __Orthogonalität beim Dekodieren__  [Rolinek et al, 2019]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<style>\n",
    "td {\n",
    "  font-size: 15px\n",
    "}\n",
    "</style>\n",
    "\n",
    "# Vergleich von PCA und DVAE\n",
    "\n",
    "### Lernen der Mannigfaltigkeit\n",
    "\n",
    "|__PCA__|__DVAE__|\n",
    "|:-----|:---|\n",
    "| Kodierung/Dekodierung, keine Robustheit gegen Rauschen | nicht-linear, probabilistische Kodierung/Dekodierung mit Robustheit gegen Rauschen und nicht-linearen Aktivierungsfunktionen|\n",
    "| unkorrelierte Koordinaten | korrelierte Ausgansdaten an der dünnsten Netzwerkebene |\n",
    "| Koordinaten sind in absteigener Reihenfolge der Varianz geordnet | Koordinaten sind ungeordnet |\n",
    "| die Spalten der Transformations-Matrix sind orthonormal | die Spalten der Transformations-Matrix sind nicht notwendigerweise orthonormal |\n",
    "| Robustheit gegenüber moderatem Rauschen mit bekannten Verteilungen | Robustheit gegen eine Vielzahl verschiedener Arten und Größenordnungen an injeziertem Rauschen (masking noise, Gaussian noise, salt-and-pepper noise), da das Entrauschen entscheidung für die Generalisierung ist |\n",
    "| einfacher Algorithmus (ohne Regularisierung), geringe Robustheit | die Punkte in niedrig-dimensionalen Mannifaltigkeiten sind robust gegen Rauschen im hoch-dimensionalen Beobachtungs-Raum |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<style>\n",
    "td {\n",
    "  font-size: 15px\n",
    "}\n",
    "</style>\n",
    "# Vergleich zwischen PCA und DVAE\n",
    "\n",
    "### Training \n",
    "\n",
    "|__PCA__|__DVAE__|\n",
    "|:-----|:---|\n",
    "| Abbildung der Eingangsdaten auf einen festen Vektor | Abbildung der Eingangsdaten auf eine Wahrscheinlichkeitsverteilung |\n",
    "| iterative Methoden: QR Zerlegung, Jacobi Algorithmus, Singulärwertzerlegung | Fehlerrückführung (Backpropagation)  |\n",
    "| aufgrund der Kovarianz-Berechnung ineffizient bei großen Datenmengen | effizient bei großen Datenmengen aufgrund der starken Fähigkeit des Erlernens der Mannigfaltigkeit |\n",
    "| basiert auf der Korrelations-/Kovarianz-Matrix, welche - zumindest in der Theorie - sehr empfindlich gegenüber Ausreißern sein kann | kann Beispiele direkt aus dem Eingangsraum generieren und daher die Eigenschfaten des Eingangsrauschens beschreiben (\"reparametrization trick\") |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lieraturverzeichnis\n",
    "\n",
    "[Deisenroth et al., 2020] Deisenroth, Marc Peter, A. Aldo Faisal, and Cheng Soon Ong. Mathematics for machine learning. Cambridge University Press, 2020.\n",
    "\n",
    "[Goodfellow et al., 2016] Ian Goodfellow, Yoshua Bengio and Aaron Courville, Deep Learning, MIT Press, 2016.\n",
    "\n",
    "[Friedman et al., 2017] Jerome H. Friedman, Robert Tibshirani, and Trevor Hastie, The Elements of Statistical Learning: Data Mining, Inference, and Prediction, Springer, 2017.\n",
    "\n",
    "[Plaut, 2018] Plaut, E., 2018. From principal subspaces to principal components with linear autoencoders. arXiv preprint arXiv:1804.10253.\n",
    "\n",
    "[Im, Bengio et al., 2017] Im, D.I.J., Ahn, S., Memisevic, R. and Bengio, Y., 2017, February. Denoising criterion for variational auto-encoding framework. In Thirty-First AAAI Conference on Artificial Intelligence.\n",
    "\n",
    "[Rolinek et al, 2019] Rolinek, M., Zietlow, D. and Martius, G., 2019. Variational Autoencoders Pursue PCA Directions (by Accident). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 12406-12415).\n",
    "\n",
    "[Lei et al., 2018] Lei, N., Luo, Z., Yau, S.T. and Gu, D.X., 2018. Geometric understanding of deep learning. arXiv preprint arXiv:1805.10451.\n",
    "\n",
    "[Kingma et al., 2013] Kingma, D.P. and Welling, M., 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Mehr details über PCA (aka Karhunen - Loeve Transform)\n",
    "\n",
    "* __Unüberwachtes__ Lernen\n",
    "* __Lineare Transformation__\n",
    "![](pca-intuition.png)\n",
    "* __\"Transformiere\"__ eine Menge von Beobachtungen in ein __anderes Koordinatensystem__, in dem die Werte der ersten Koordinate (Komponente) die __größtmögliche Varianz__ aufweisen [Friedman et al., 2017]\n",
    "* Die __resultierenden Koordinaten (Komponenten)__ sind __nicht__ mit den ursprünglichen Koordinaten __korreliert__    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verbesserter PCA Algorithmus für hochdimensionale Datensätze\n",
    "\n",
    "Manchmal kann die Dimensionalität unseres Datensatzes größer sein als die Anzahl der Stichproben, die wir haben. Dann könnte es ineffizient sein, PCA mit unserer obigen Implementierung durchzuführen. Stattdessen, können wir die PCA auf eine effizientere Weise implementieren, die wir \"PCA für hochdimensionale Daten\" (PCA_high_dim) nennen.\n",
    "\n",
    "Nachfolgend sind die Schritte zur Durchführung von PCA für hochdimensionale Datensätze aufgeführt\n",
    "1. Berechnen Sie die Matrix $\\boldsymbol X\\boldsymbol X^T$ (eine $N$ mal $N$ Matrix mit $N \\ll D$)\n",
    "2. Berechnen Sie Eigenwerte $\\lambda$s und Eigenvektoren $V$ für $\\boldsymbol X\\boldsymbol X^T$\n",
    "3) Berechnen Sie die Eigenvektoren der ursprünglichen Kovarianzmatrix als $\\boldsymbol X^T\\boldsymbol V$. Wählen Sie die Eigenvektoren, die mit den M größten Eigenwerten assoziiert sind, als Basis für den Hauptunterraum $U$.\n",
    "4) Berechnen Sie die orthogonale Projektion der Daten auf den Unterraum, der von den Spalten von $\\boldsymbol U$ aufgespannt wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PCA für hochdimensionale Datensätze\n",
    "\n",
    "def PCA_high_dim(X, n_components):\n",
    "    \"\"\"Berechnen Sie PCA für kleine Stichprobengrößen, aber hochdimensionale Merkmale. \n",
    "    Args:\n",
    "        X: nDarray der Größe (N, D), wobei D die Dimension der Stichprobe ist,\n",
    "            und N die Anzahl der Stichproben ist\n",
    "        num_components: die Anzahl der zu verwendenden Hauptkomponenten.\n",
    "    Returns:\n",
    "        X_reconstruct: (N, D) ndarray. die Rekonstruktion\n",
    "            von X aus den ersten `num_components`-Prinzipalkomponenten.\n",
    "    \"\"\"\n",
    "    N, D = X.shape\n",
    "    M = np.dot(X, X.T) / N\n",
    "    eig_vals, eig_vecs = eig(M)\n",
    "    eig_vals, eig_vecs = eig_vals[:n_components], eig_vecs[:, :n_components]\n",
    "    U = (X.T @ (eig_vecs))\n",
    "    reconstruction = np.zeros((N, D))\n",
    "    reconstruction = ((U @ np.linalg.inv(U.T @ U) @ U.T) @ X.T).T\n",
    "    return reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wir können eine Standardlösung berechnen, die von der \n",
    "    # PCA-Implementierung von scikit-learn vorgegeben wird\n",
    "pca = SKPCA(n_components=60, svd_solver='full')\n",
    "sklearn_reconst = pca.inverse_transform(pca.fit_transform(Xbar))\n",
    "reconst = PCA_high_dim(Xbar, 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(sklearn_reconst[5], [8, 8]), cmap='gray');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(np.reshape(reconst[5].real, [8, 8]), cmap='gray');"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
